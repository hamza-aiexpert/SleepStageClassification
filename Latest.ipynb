{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import pyedflib\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder = 'ISRUC DATASET SUBGROUP 1 EXTRACTED'\n",
    "signals_to_check = 8\n",
    "no_of_subjects = 30\n",
    "sensor_values_count = 5280000\n",
    "labels_values_count = 880\n",
    "frequency_of_each_sensor = 200 #In hertz means (200 values in 1 second)\n",
    "epoch_size = 30 #In Seconds\n",
    "\n",
    "#Awake\n",
    "Awake = 0\n",
    "#Non Rapid Eye Movement\n",
    "N1 = 1\n",
    "N2 = 2\n",
    "N3 = 3\n",
    "#Rapid Eye Movement\n",
    "R = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:51<00:00,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to execute:  111.26752758026123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# After Executing Ram Usage 12.6GB\n",
    "# Save Memory Loading\n",
    "start_time = time.time()\n",
    "\n",
    "X_tensor = None\n",
    "y_tensor = None\n",
    "\n",
    "for i in tqdm(range(1, no_of_subjects + 1)):\n",
    "    subject_path = os.path.join(folder, str(i))\n",
    "\n",
    "    # Load reshaped data and labels directly into tensors\n",
    "    input_data = torch.tensor(np.load(os.path.join(subject_path, \"input.npy\")), dtype=torch.float32)\n",
    "    labels = torch.tensor(np.load(os.path.join(subject_path, \"labels.npy\")), dtype=torch.int8)\n",
    "\n",
    "    # Concatenate data to tensors\n",
    "    if X_tensor is None:\n",
    "        X_tensor = input_data\n",
    "        y_tensor = labels\n",
    "    else:\n",
    "        X_tensor = torch.cat((X_tensor, input_data), dim=0)\n",
    "        y_tensor = torch.cat((y_tensor, labels), dim=0)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(split_ratio * len(X_tensor))\n",
    "train_input_data, test_input_data = X_tensor[:split_index], X_tensor[split_index:]\n",
    "train_labels, test_labels = y_tensor[:split_index], y_tensor[split_index:]\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to execute: \", elapsed_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have \n",
    "# Train data = 14080    1 Batch = 14080/batch_size\n",
    "# Test data = 3520      1 Batch = 3520/batch_size\n",
    "\n",
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "# Convert data to PyTorch tensors\n",
    "#X_tensor = torch.tensor(all_input_data, dtype=torch.float32)\n",
    "#y_tensor = torch.tensor(all_labels, dtype=torch.int32) #Previously using int32\n",
    "\n",
    "# Define dataloaders for training and testing\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#test_size = len(dataset) - train_size\n",
    "#train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#train_features, train_labels = next(iter(train_loader))\n",
    "\n",
    "\n",
    " # First split to get training (80%) and remaining (20%)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, stratify=y_tensor)\n",
    "\n",
    "# Further split remaining into validation (10%) and test (10%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, stratify=y_train_val)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MyCustomDataset(X_train, y_train)\n",
    "val_dataset = MyCustomDataset(X_val, y_val)\n",
    "test_dataset = MyCustomDataset(X_test, y_test)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "torch.cuda.empty_cache()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run training  on unbalanced dataset\n",
    "# Apply data augmentation to balanced the dataset \n",
    "# Training on balanced dataset \n",
    "# epoch size 100\n",
    "# Folds 5\n",
    "# Without Bsmote => Noise Injection\n",
    "# Data Augmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL STRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SleepStageClassifier(nn.Module):\n",
    "    def __init__(self, input_channels=8, num_classes=6):\n",
    "        super(SleepStageClassifier, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        self.batchnorm1 = nn.BatchNorm1d(16)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(32)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        # Max pooling layers\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * (6000 // 8), 1024)  # Adjust input size based on the actual input shape\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with batch normalization, ReLU activation, and max pooling\n",
    "        \n",
    "        x = self.pool(nn.functional.relu(self.batchnorm1(self.conv1(x))))\n",
    "        x = self.pool(nn.functional.relu(self.batchnorm2(self.conv2(x))))\n",
    "        x = self.pool(nn.functional.relu(self.batchnorm3(self.conv3(x))))\n",
    "\n",
    "        # Flatten the output for fully connected layers\n",
    "        x = x.view(-1, 64 * (6000 // 8))\n",
    "        \n",
    "        # Fully connected layers with dropout\n",
    "        x = self.dropout(nn.functional.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filepath):\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    print(f\"Model saved successfully at {filepath}\")\n",
    "\n",
    "def load_model(model, filepath, device='cuda'):\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    model.to(device)\n",
    "    print(f\"Model loaded successfully from {filepath}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNBALANCED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution in Training Data:\n",
      "Class 0: 8073 samples (30.58%)\n",
      "Class 1: 3268 samples (12.38%)\n",
      "Class 2: 7000 samples (26.52%)\n",
      "Class 3: 4704 samples (17.82%)\n",
      "Class 5: 3355 samples (12.71%)\n"
     ]
    }
   ],
   "source": [
    "# Calculate class distribution in training data\n",
    "class_distribution = Counter(y_tensor.tolist())\n",
    "\n",
    "# Calculate percentage of each class\n",
    "total_samples = len(y_tensor)\n",
    "class_percentage = {label: count / total_samples * 100 for label, count in class_distribution.items()}\n",
    "\n",
    "# Display class distribution\n",
    "print(\"Class Distribution in Training Data:\")\n",
    "for label, count in class_distribution.items():\n",
    "    print(f\"Class {label}: {count} samples ({class_percentage[label]:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [02:31<4:10:16, 151.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.9361, Accuracy: 0.4228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [05:21<4:25:31, 162.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Loss: 1.1596, Accuracy: 0.5352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [08:16<4:31:47, 168.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Loss: 1.0627, Accuracy: 0.5827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [11:11<4:33:06, 170.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Loss: 0.9994, Accuracy: 0.6102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [14:05<4:32:30, 172.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 0.9473, Accuracy: 0.6316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [17:00<4:31:05, 173.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Loss: 0.9081, Accuracy: 0.6460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [19:56<4:29:21, 173.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Loss: 0.8842, Accuracy: 0.6590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [22:51<4:27:31, 174.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Loss: 0.8384, Accuracy: 0.6765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [23:21<4:28:41, 175.23s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 50\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     51\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     52\u001b[0m correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Normal Training\n",
    "\n",
    "learning_rate = 0.001\n",
    "# Define the number of folds for stratified CV\n",
    "num_folds = 5\n",
    "\n",
    "# Initialize stratified KFold\n",
    "stratified_kfold = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# Training loop with stratified cross-validation\n",
    "for fold, (train_indices, val_indices) in enumerate(stratified_kfold.split(X_tensor, y_tensor)):\n",
    "    print(f'Fold {fold + 1}/{num_folds}')\n",
    "    \n",
    "    # Initialize model, optimizer, criterion\n",
    "    model = SleepStageClassifier()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Move model to appropriate device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            inputs = inputs.permute(0, 2, 1)\n",
    "            #print(labels.dtype)\n",
    "            # Convert target labels to torch.int64\n",
    "            labels = labels.to(torch.int64)\n",
    "\n",
    "            # Ensure model outputs have the correct data type\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.float()  # Convert to float if needed\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #print(outputs.dtype)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = correct_predictions / total_samples\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "            val_inputs = val_inputs.permute(0, 2, 1)\n",
    "\n",
    "            val_outputs = model(val_inputs)\n",
    "            _, val_predicted = torch.max(val_outputs, 1)\n",
    "            val_correct_predictions += (val_predicted == val_labels).sum().item()\n",
    "            val_total_samples += val_labels.size(0)\n",
    "\n",
    "    val_accuracy = val_correct_predictions / val_total_samples\n",
    "    print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "print('Training finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "# Training with Stages\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Function to save the state\n",
    "def save_state(fold, model_state, optimizer_state):\n",
    "    state = {\n",
    "        'fold': fold,\n",
    "        'model_state': model_state,\n",
    "        'optimizer_state': optimizer_state\n",
    "    }\n",
    "    with open('training_state.pkl', 'wb') as f:\n",
    "        pickle.dump(state, f)\n",
    "\n",
    "# Function to load the state\n",
    "def load_state():\n",
    "    if os.path.exists('training_state.pkl'):\n",
    "        with open('training_state.pkl', 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "            return state\n",
    "    return None\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_folds = 5\n",
    "\n",
    "# Assuming X_tensor and y_tensor are your data tensors\n",
    "# And dataset is your full dataset\n",
    "# Example: dataset = MyCustomDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Initialize stratified KFold\n",
    "stratified_kfold = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# Load previous state if available\n",
    "state = load_state()\n",
    "if state:\n",
    "    start_fold = state['fold']\n",
    "else:\n",
    "    start_fold = 0\n",
    "\n",
    "# Training loop with stratified cross-validation\n",
    "for fold, (train_indices, val_indices) in enumerate(stratified_kfold.split(X_tensor, y_tensor)):\n",
    "    if fold < start_fold:\n",
    "        continue\n",
    "\n",
    "    print(f'Fold {fold + 1}/{num_folds}')\n",
    "    \n",
    "    # Initialize model, optimizer, criterion\n",
    "    model = SleepStageClassifier()\n",
    "    model.to(device)  # Move model to device here\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if state and fold == start_fold:\n",
    "        model.load_state_dict(state['model_state'])\n",
    "        optimizer.load_state_dict(state['optimizer_state'])\n",
    "    \n",
    "    \n",
    "    # Initialize data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            inputs = inputs.permute(0, 2, 1)\n",
    "            labels = labels.to(torch.int64)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.float()  # Convert to float if needed\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = correct_predictions / total_samples\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "            val_inputs = val_inputs.permute(0, 2, 1)\n",
    "\n",
    "            val_outputs = model(val_inputs)\n",
    "            _, val_predicted = torch.max(val_outputs, 1)\n",
    "            val_correct_predictions += (val_predicted == val_labels).sum().item()\n",
    "            val_total_samples += val_labels.size(0)\n",
    "\n",
    "    val_accuracy = val_correct_predictions / val_total_samples\n",
    "    print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # Save the state\n",
    "    save_state(fold + 1, model.state_dict(), optimizer.state_dict())\n",
    "\n",
    "    # Ask if the user wants to continue\n",
    "    cont = input(f'Fold {fold + 1} completed. Do you want to continue to the next fold? (yes/no): ')\n",
    "    if cont.lower() != 'yes':\n",
    "        print('Training stopped.')\n",
    "        break\n",
    "\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SleepStageClassifier(\n",
       "  (conv1): Conv1d(8, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv2): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv3): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (batchnorm1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=48000, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=6, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to load the state\n",
    "def load_state():\n",
    "    if os.path.exists('training_state.pkl'):\n",
    "        with open('training_state.pkl', 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "            return state\n",
    "    return None\n",
    "\n",
    "# Initialize the model and load the saved state\n",
    "model = SleepStageClassifier()\n",
    "state = load_state()\n",
    "if state:\n",
    "    model.load_state_dict(state['model_state'])\n",
    "model.to(device)  # Move model to the appropriate device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully at SleepStageClassifier.pth\n"
     ]
    }
   ],
   "source": [
    "save_model(model, \"SleepStageClassifier.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BALANCED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [01:59<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Dataset\n",
      "Input data shape: (44877, 6000, 8)\n",
      "Labels shape: (44877,)\n"
     ]
    }
   ],
   "source": [
    "# Define the batch size for processing\n",
    "batch_size = 512\n",
    "\n",
    "# Create an instance of B-SMOTE\n",
    "bsmote = BorderlineSMOTE()\n",
    "\n",
    "# Process each batch sequentially\n",
    "balanced_batches_X = []\n",
    "balanced_batches_y = []\n",
    "\n",
    "for i in tqdm(range(0, len(X_tensor), batch_size)):\n",
    "    batch_X = X_tensor[i:i+batch_size]\n",
    "    batch_y = y_tensor[i:i+batch_size]\n",
    "\n",
    "    # Reshape input data for B-SMOTE\n",
    "    batch_X_reshaped = batch_X.reshape(-1, batch_X.shape[1] * batch_X.shape[2])\n",
    "\n",
    "    # Apply B-SMOTE to balance the classes for the current batch\n",
    "    batch_X_balanced, batch_y_balanced = bsmote.fit_resample(batch_X_reshaped, batch_y)\n",
    "\n",
    "    # Reshape the balanced batch back to its original shape\n",
    "    batch_X_balanced = batch_X_balanced.reshape(-1, batch_X.shape[1], batch_X.shape[2])\n",
    "\n",
    "    # Store the balanced batch\n",
    "    balanced_batches_X.append(batch_X_balanced)\n",
    "    balanced_batches_y.append(batch_y_balanced)\n",
    "\n",
    "# Concatenate the balanced batches\n",
    "X_balanced = np.concatenate(balanced_batches_X, axis=0)\n",
    "y_balanced = np.concatenate(balanced_batches_y, axis=0)\n",
    "\n",
    "# Display the balanced data shapes\n",
    "print(\"Balanced Dataset\")\n",
    "print(\"Input data shape:\", X_balanced.shape)\n",
    "print(\"Labels shape:\", y_balanced.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution in Training Data:\n",
      "Class 0: 10712 samples (23.87%)\n",
      "Class 1: 10459 samples (23.31%)\n",
      "Class 2: 9633 samples (21.47%)\n",
      "Class 3: 6239 samples (13.90%)\n",
      "Class 5: 7834 samples (17.46%)\n"
     ]
    }
   ],
   "source": [
    "# Calculate class distribution in training data\n",
    "class_distribution = Counter(y_balanced.tolist())\n",
    "\n",
    "# Calculate percentage of each class\n",
    "total_samples = len(y_balanced)\n",
    "class_percentage = {label: count / total_samples * 100 for label, count in class_distribution.items()}\n",
    "\n",
    "# Display class distribution\n",
    "print(\"Class Distribution in Training Data:\")\n",
    "for label, count in class_distribution.items():\n",
    "    print(f\"Class {label}: {count} samples ({class_percentage[label]:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [02:02<3:21:42, 122.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.2381, Accuracy: 0.9324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [04:03<3:18:14, 121.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Loss: 0.1656, Accuracy: 0.9485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [06:03<3:15:34, 120.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Loss: 0.1422, Accuracy: 0.9556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [08:04<3:13:21, 120.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Loss: 0.1234, Accuracy: 0.9591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [10:04<3:11:15, 120.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 0.1101, Accuracy: 0.9628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [12:05<3:09:09, 120.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Loss: 0.1024, Accuracy: 0.9660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [14:06<3:07:03, 120.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Loss: 0.0864, Accuracy: 0.9708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [16:06<3:04:58, 120.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Loss: 0.0912, Accuracy: 0.9687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [18:07<3:02:54, 120.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Loss: 0.0851, Accuracy: 0.9707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [20:07<3:00:53, 120.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.0854, Accuracy: 0.9716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [22:08<2:58:51, 120.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100], Loss: 0.0788, Accuracy: 0.9731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [24:08<2:56:51, 120.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100], Loss: 0.0857, Accuracy: 0.9710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [26:09<2:54:52, 120.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100], Loss: 0.0839, Accuracy: 0.9720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [28:10<2:52:52, 120.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100], Loss: 0.0718, Accuracy: 0.9745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [30:10<2:50:50, 120.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100], Loss: 0.0757, Accuracy: 0.9741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [32:11<2:48:50, 120.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100], Loss: 0.0749, Accuracy: 0.9747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [34:11<2:46:49, 120.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100], Loss: 0.0726, Accuracy: 0.9744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [36:12<2:44:47, 120.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100], Loss: 0.0702, Accuracy: 0.9764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [38:13<2:42:47, 120.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100], Loss: 0.0716, Accuracy: 0.9760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [40:13<2:40:47, 120.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Loss: 0.0671, Accuracy: 0.9770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [42:14<2:38:51, 120.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100], Loss: 0.0741, Accuracy: 0.9753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [44:14<2:36:46, 120.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100], Loss: 0.0629, Accuracy: 0.9778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [46:15<2:34:42, 120.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100], Loss: 0.0668, Accuracy: 0.9776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [48:15<2:32:39, 120.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100], Loss: 0.0716, Accuracy: 0.9770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [50:16<2:30:51, 120.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100], Loss: 0.0668, Accuracy: 0.9770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [52:17<2:28:46, 120.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100], Loss: 0.0692, Accuracy: 0.9757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [54:17<2:26:41, 120.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100], Loss: 0.0640, Accuracy: 0.9786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [56:18<2:24:37, 120.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100], Loss: 0.0780, Accuracy: 0.9749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [58:18<2:22:35, 120.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100], Loss: 0.0591, Accuracy: 0.9801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [1:00:19<2:20:34, 120.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100], Loss: 0.0613, Accuracy: 0.9796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [1:02:19<2:18:33, 120.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100], Loss: 0.0644, Accuracy: 0.9784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [1:04:19<2:16:31, 120.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100], Loss: 0.0639, Accuracy: 0.9786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [1:06:20<2:14:31, 120.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100], Loss: 0.0610, Accuracy: 0.9794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [1:08:21<2:12:32, 120.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100], Loss: 0.0646, Accuracy: 0.9788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [1:10:21<2:10:32, 120.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100], Loss: 0.0603, Accuracy: 0.9797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [1:12:22<2:08:32, 120.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100], Loss: 0.0590, Accuracy: 0.9798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [1:14:22<2:06:36, 120.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/100], Loss: 0.0621, Accuracy: 0.9796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [1:16:23<2:04:34, 120.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100], Loss: 0.0603, Accuracy: 0.9784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [1:18:23<2:02:32, 120.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/100], Loss: 0.0637, Accuracy: 0.9789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [1:20:24<2:00:30, 120.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100], Loss: 0.0579, Accuracy: 0.9803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [1:22:24<1:58:29, 120.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/100], Loss: 0.0661, Accuracy: 0.9800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [1:24:25<1:56:28, 120.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/100], Loss: 0.0613, Accuracy: 0.9796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [1:26:25<1:54:29, 120.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/100], Loss: 0.0670, Accuracy: 0.9790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [1:28:26<1:52:28, 120.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100], Loss: 0.0597, Accuracy: 0.9811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [1:30:26<1:50:27, 120.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/100], Loss: 0.0592, Accuracy: 0.9803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [1:32:27<1:48:27, 120.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/100], Loss: 0.0684, Accuracy: 0.9787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [1:34:27<1:46:27, 120.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/100], Loss: 0.0626, Accuracy: 0.9801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [1:36:28<1:44:25, 120.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/100], Loss: 0.0545, Accuracy: 0.9813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [1:38:28<1:42:24, 120.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/100], Loss: 0.0712, Accuracy: 0.9784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [1:40:29<1:40:24, 120.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100], Loss: 0.0609, Accuracy: 0.9790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [1:42:29<1:38:22, 120.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/100], Loss: 0.0582, Accuracy: 0.9806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [1:44:30<1:36:23, 120.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/100], Loss: 0.0560, Accuracy: 0.9822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [1:46:30<1:34:22, 120.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/100], Loss: 0.0538, Accuracy: 0.9820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [1:48:30<1:32:20, 120.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/100], Loss: 0.0599, Accuracy: 0.9806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [1:50:31<1:30:20, 120.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/100], Loss: 0.0565, Accuracy: 0.9811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [1:52:31<1:28:19, 120.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/100], Loss: 0.0615, Accuracy: 0.9809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [1:54:32<1:26:18, 120.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/100], Loss: 0.0495, Accuracy: 0.9832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [1:56:32<1:24:18, 120.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/100], Loss: 0.0550, Accuracy: 0.9818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [1:58:33<1:22:17, 120.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/100], Loss: 0.0607, Accuracy: 0.9803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [2:00:33<1:20:18, 120.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/100], Loss: 0.0614, Accuracy: 0.9792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [2:02:34<1:18:18, 120.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/100], Loss: 0.0562, Accuracy: 0.9811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [2:04:34<1:16:20, 120.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/100], Loss: 0.0571, Accuracy: 0.9814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [2:06:35<1:14:19, 120.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/100], Loss: 0.0553, Accuracy: 0.9813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [2:08:35<1:12:19, 120.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/100], Loss: 0.0534, Accuracy: 0.9825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [2:10:36<1:10:18, 120.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/100], Loss: 0.0531, Accuracy: 0.9827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [2:12:36<1:08:16, 120.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/100], Loss: 0.0562, Accuracy: 0.9827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [2:14:37<1:06:15, 120.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67/100], Loss: 0.0586, Accuracy: 0.9811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [2:16:37<1:04:14, 120.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/100], Loss: 0.0539, Accuracy: 0.9821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69/100 [2:18:38<1:02:14, 120.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [69/100], Loss: 0.0546, Accuracy: 0.9816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [2:20:38<1:00:14, 120.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/100], Loss: 0.0578, Accuracy: 0.9811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [2:22:39<58:14, 120.48s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/100], Loss: 0.0602, Accuracy: 0.9806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [2:24:39<56:13, 120.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/100], Loss: 0.0591, Accuracy: 0.9809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [2:26:40<54:12, 120.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [73/100], Loss: 0.0574, Accuracy: 0.9811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [2:28:40<52:12, 120.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/100], Loss: 0.0575, Accuracy: 0.9818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [2:30:40<50:11, 120.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/100], Loss: 0.0541, Accuracy: 0.9830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [2:32:41<48:11, 120.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/100], Loss: 0.0553, Accuracy: 0.9820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [2:34:42<46:11, 120.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/100], Loss: 0.0624, Accuracy: 0.9809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [2:36:42<44:10, 120.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [78/100], Loss: 0.0520, Accuracy: 0.9828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [2:38:43<42:10, 120.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [79/100], Loss: 0.0548, Accuracy: 0.9829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [2:40:43<40:10, 120.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/100], Loss: 0.0535, Accuracy: 0.9824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [2:42:43<38:06, 120.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [81/100], Loss: 0.0726, Accuracy: 0.9789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [2:44:43<36:04, 120.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82/100], Loss: 0.0551, Accuracy: 0.9820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [2:46:43<34:02, 120.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [83/100], Loss: 0.0501, Accuracy: 0.9830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [2:48:43<32:01, 120.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [84/100], Loss: 0.0544, Accuracy: 0.9819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [2:50:43<30:00, 120.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [85/100], Loss: 0.0519, Accuracy: 0.9834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [2:52:43<28:00, 120.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [86/100], Loss: 0.0477, Accuracy: 0.9832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [2:54:43<25:59, 119.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [87/100], Loss: 0.0533, Accuracy: 0.9828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88/100 [2:56:43<23:59, 119.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [88/100], Loss: 0.0548, Accuracy: 0.9820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89/100 [2:58:43<21:59, 120.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [89/100], Loss: 0.0492, Accuracy: 0.9826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [3:00:43<19:59, 119.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/100], Loss: 0.0571, Accuracy: 0.9812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [3:02:43<17:59, 119.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [91/100], Loss: 0.0542, Accuracy: 0.9836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [3:04:42<15:59, 119.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/100], Loss: 0.0535, Accuracy: 0.9830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [3:06:42<13:59, 119.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [93/100], Loss: 0.0537, Accuracy: 0.9826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [3:08:42<11:59, 119.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [94/100], Loss: 0.0503, Accuracy: 0.9840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [3:10:42<09:59, 119.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [95/100], Loss: 0.0593, Accuracy: 0.9823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [3:12:42<07:59, 119.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [96/100], Loss: 0.0559, Accuracy: 0.9829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97/100 [3:14:42<05:59, 119.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [97/100], Loss: 0.0540, Accuracy: 0.9822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [3:16:42<03:59, 119.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [98/100], Loss: 0.0537, Accuracy: 0.9830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99/100 [3:18:42<01:59, 119.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [99/100], Loss: 0.0565, Accuracy: 0.9825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [3:20:42<00:00, 120.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0540, Accuracy: 0.9830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9364\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "\n",
    "# Convert balanced data to PyTorch tensors\n",
    "X_balanced_tensor = torch.tensor(X_balanced, dtype=torch.float32)\n",
    "y_balanced_tensor = torch.tensor(y_balanced, dtype=torch.long)  # Use torch.long for labels\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define dataset and dataloaders\n",
    "batch_size = 32  # You can adjust the batch size as needed\n",
    "num_epochs = 100\n",
    "# Create TensorDataset\n",
    "balanced_dataset = TensorDataset(X_balanced_tensor, y_balanced_tensor)\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_size = int(0.8 * len(balanced_dataset))\n",
    "test_size = len(balanced_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(balanced_dataset, [train_size, test_size])\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define the number of folds for stratified CV\n",
    "num_folds = 5\n",
    "\n",
    "# Function to save the state\n",
    "def save_state(fold, model_state, optimizer_state):\n",
    "    state = {\n",
    "        'fold': fold,\n",
    "        'model_state': model_state,\n",
    "        'optimizer_state': optimizer_state\n",
    "    }\n",
    "    with open('training_state_balanced.pkl', 'wb') as f:\n",
    "        pickle.dump(state, f)\n",
    "\n",
    "# Function to load the state\n",
    "def load_state():\n",
    "    if os.path.exists('training_state_balanced.pkl'):\n",
    "        with open('training_state_balanced.pkl', 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "            return state\n",
    "    return None\n",
    "\n",
    "# Load previous state if available\n",
    "state = load_state()\n",
    "if state:\n",
    "    start_fold = state['fold']\n",
    "else:\n",
    "    start_fold = 0\n",
    "\n",
    "# Initialize stratified KFold\n",
    "stratified_kfold = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# Training loop with stratified cross-validation\n",
    "for fold, (train_indices, val_indices) in enumerate(stratified_kfold.split(X_balanced_tensor, y_balanced_tensor)):\n",
    "    if fold < start_fold:\n",
    "        continue\n",
    "\n",
    "    print(f'Fold {fold + 1}/{num_folds}')\n",
    "    \n",
    "    # Initialize model, optimizer, criterion\n",
    "    model = SleepStageClassifier()\n",
    "    model.to(device)  # Move model to device here\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if state and fold == start_fold:\n",
    "        model.load_state_dict(state['model_state'])\n",
    "        optimizer.load_state_dict(state['optimizer_state'])\n",
    "    \n",
    "    # Initialize data loaders with balanced dataset\n",
    "    train_subset = Subset(balanced_dataset, train_indices)\n",
    "    val_subset = Subset(balanced_dataset, val_indices)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            inputs = inputs.permute(0, 2, 1)\n",
    "            labels = labels.to(torch.int64)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.float()  # Convert to float if needed\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = correct_predictions / total_samples\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "            val_inputs = val_inputs.permute(0, 2, 1)\n",
    "\n",
    "            val_outputs = model(val_inputs)\n",
    "            _, val_predicted = torch.max(val_outputs, 1)\n",
    "            val_correct_predictions += (val_predicted == val_labels).sum().item()\n",
    "            val_total_samples += val_labels.size(0)\n",
    "\n",
    "    val_accuracy = val_correct_predictions / val_total_samples\n",
    "    print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # Save the state\n",
    "    save_state(fold + 1, model.state_dict(), optimizer.state_dict())\n",
    "\n",
    "    # Ask if the user wants to continue\n",
    "    cont = input(f'Fold {fold + 1} completed. Do you want to continue to the next fold? (yes/no): ')\n",
    "    if cont.lower() != 'yes':\n",
    "        print('Training stopped.')\n",
    "        break\n",
    "\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully at SleepStageClassifier_balanced.pth\n"
     ]
    }
   ],
   "source": [
    "save_model(model, \"SleepStageClassifier_balanced.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from SleepStageClassifier_balanced.pth\n",
      "Test Accuracy: 0.9790\n",
      "Precision: 0.9752\n",
      "Recall: 0.9784\n",
      "F1-score: 0.9767\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to numpy.ndarray.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1-score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1New-score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1New\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKappa coefficient: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkappa\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to numpy.ndarray.__format__"
     ]
    }
   ],
   "source": [
    "model = load_model(SleepStageClassifier(), \"SleepStageClassifier_balanced.pth\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        inputs = inputs.permute(0, 2, 1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate predicted labels\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Update true and predicted labels\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "recall = recall_score(true_labels, predicted_labels, average='macro')\n",
    "f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "f1New = f1_score(true_labels, predicted_labels, average=None)\n",
    "\n",
    "#f1 = f1_score(true_labels, predicted_labels, average='none')\n",
    "\n",
    "kappa = cohen_kappa_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-score: {f1:.4f}')\n",
    "print(f'F1New-score: {f1New:.4f}')\n",
    "\n",
    "print(f'Kappa coefficient: {kappa:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from SleepStageClassifier.pth\n",
      "Test Accuracy: 0.8865\n",
      "Precision: 0.8884\n",
      "Recall: 0.8912\n",
      "F1-score: 0.8885\n",
      "Kappa coefficient: 0.8568\n"
     ]
    }
   ],
   "source": [
    "model = load_model(SleepStageClassifier(), \"SleepStageClassifier.pth\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        inputs = inputs.permute(0, 2, 1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate predicted labels\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Update true and predicted labels\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "recall = recall_score(true_labels, predicted_labels, average='macro')\n",
    "f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "#f1 = f1_score(true_labels, predicted_labels, average='none')\n",
    "\n",
    "kappa = cohen_kappa_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-score: {f1:.4f}')\n",
    "print(f'Kappa coefficient: {kappa:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
